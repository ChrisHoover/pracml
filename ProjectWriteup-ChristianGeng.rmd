---
title: "Assigment Writeup"
author: "Christian Geng"
date: "09/22/2015"
output: html_document
---

# Let's do some looney changes!

Haha

# Introduction : Objective

The objective of the project is to build a machine learning model. 
The dataset is introduced on the webpage [here][1], and references a publication by Ugulino et. al. describing the dataset. It contains raw data and derived data from wearable devices attached to a belt around the waste, the arm, the dumbbell itself and a  glove. The participants were engaged in a fitness  exercise called "Unilateral Dumbbell Biceps Curl" (see Velloso et. al. (2013)).  

The objective of the project is to predict a variable "classe"  in the training set, use cross validation, and give the  expected out of sample error, and motivate the choices. Also, 20 different test cases have to be predicted.  


# Prepare R and the Dataset for Modelling

Load the dataset, the libraries in order to get started ...

```{r loadlibs,echo=TRUE,results='hide'}
library(lattice)
library(ggplot2)
#library(randomForest)
library(caret)
library(xtable)
library(plyr)
indir <- ("/D/myfiles/2015/courseraAppliedMachineLearning/project/")
datdir <- paste(indir,"/data/",sep="")
source(paste(indir,'functions.r',sep='')) # load additional functions
set.seed(33833)
```

```{r load Data,echo=FALSE,results='markup'}
trainingfilename="pml-training.csv"
#train_data <- read.table(paste(datdir,trainingfilename,sep=""), header=TRUE ,sep=",")
train_data <- read.table(paste(datdir,trainingfilename,sep=""), header=TRUE ,sep=",",na.strings=c("NA",""))

testingfilename="pml-testing.csv"
oobdata <- read.table(paste(datdir,testingfilename,sep=""), header=TRUE ,sep=",")
```



# Select Variables
## Remove descriptor variables (first seven variables)
The first few variables seem to be junk, so remove them:
```{r Remove junk,echo=TRUE,results='markup'}
  nvarsbefore <- dim(train_data)[2] 
  train_data <- train_data[-(1:7)]
  oobdata <- oobdata[-(1:7)]
  nvarsAfter <- dim(train_data)[2] 
```
Check: number of variables before=`r nvarsbefore`, and after: `r nvarsAfter`.

## Remove variables with only missing values in the oob test set 

I found this on the forum: If the test set (in that case the data that we are supposed to submit through the Coursera Submission system) contains variables that are missing for all cases, then you do not need to build a model on the training data containing them. So they can go ...


```{r Remove oob missing vars,echo=TRUE,results='markup'}
source(paste(indir,'functions.r',sep=''));
test = rmMissing4allcases(oobdata)
logicalVector <- apply(!is.na(oobdata),2,sum)== dim(oobdata)[1] # all vars that do not have missing everywhere
nvarsbefore <- dim(train_data)[2] 
train_data  <- train_data[,logicalVector]
nvarsAfter <- dim(train_data)[2] 
```
Number of variables before removal: `r nvarsbefore`
Number of variables before removal: `r nvarsAfter`

## Split data into:Training and Testing


```{r ncasesSplit, echo=TRUE}
  puse = .3
  TrainPercent <- puse*100
  inTrain = createDataPartition(train_data$classe, p = puse)[[1]]
  traininguse = train_data[ inTrain,]
  testinguse = train_data[-inTrain,]
  ncasesTrain <- dim(traininguse)[1]
  ncasesTest <- dim(testinguse)[1]
  ncasesAll <-dim(train_data)[1]
  #nvarsAfter <- dim(traininguse)[2] 
```
The total data set contains `r ncasesAll`
There was some discussion about this issue in the forum.
For memory reasons, I used `r TrainPercent`% of the data in the training set.
The results suggest that this works.   

This results of the split contain  `r ncasesTrain` in the training data set and `r ncasesTest` in the test data set.   



## Eliminate data with near zero variance

It is not advisable to retain variables that have almost zero variance. 
I am applying this procedure only to the numeric variables. 


```{r eliminate zero variance}
  #nsv <- nearZeroVar(train_data,saveMetrics=TRUE)
  #dim(nsv)
  #nsv <- nearZeroVar(train_data[sapply(train_data,is.numeric),],saveMetrics=TRUE)
  #numericdata <- train_data[,nearZeroVar(train_data[sapply(train_data,is.numeric),])]

  nsv <- data.frame(nearZeroVar(traininguse[sapply(traininguse,is.numeric),],saveMetrics=TRUE),sapply(traininguse,class))  
  nsv <- rename(nsv, c("sapply.traininguse..class."="type"))
  vars2drop <- row.names(nsv[nsv$type == "numeric" & nsv$nzv == TRUE ,])
  traininguse <- traininguse[setdiff(names(traininguse),vars2drop)]
  ndrop <- length(vars2drop)
  nvarsinmodel <- dim(traininguse)[2]
```

This allows to drop `r ndrop` variables. The training data will use `r nvarsinmodel` variables. As it looks, this procedure does not remove 
any further variables. 

# Modelling the Training Set

As they say in the lecture, a random forest should probably be less problematic when it comes to preprocessing: Scaling and centering are not needed. 
Also the original publication cited does use RF.   
I have not tried to replace correlated variables by Principal Components.    

I used only a small fraction of the data for training, I use cross validation splitting the data in 5 partitions. 
I had to limit the amount of training data due to memory limitations (only 30%, see above).  
Probably one could train with more data by optimizing the settings. But the results were quite good, so this did not seem necessary. 


```{r model training data}
# load the model from disk instead of running it again
modelfile <- paste(datdir,"rf_model.RData",sep='')
load(modelfile)

# MODEL CODE: NOT RUN
# rf_model<-train(classe~.,data=traininguse,method="rf",
#                trControl=trainControl(method="cv",number=5),
#                prox=TRUE,allowParallel=TRUE)
# save(rf_model,file = modelfile)

print(rf_model)
varImp(rf_model)
print(rf_model$finalModel)
```

# Predicting on the Test Set

As described above, I have used fairly little data during training.  

I report accuracy values, as we are modelling a qualitative outcome. 


```{r predictions on test data}
  predictions <- predict(rf_model,newdata=testinguse)
  confmat <- confusionMatrix(predictions,testinguse$classe)
  acc <-  as.numeric(confmat$overall[1])
```
** TEST Set  Accuracy: `r acc` **  
The cross validation error on the training set above was **0.9764007**.
So I can conclude that we got some decent fit.

# Writing the oob result files for submission

```{r oob data predictions}
answers <- predict(rf_model,newdata=oobdata)
source(paste(indir,'functions.r',sep='')) 
pml_write_files(answers,datdir)
```



# Observation

Although the results are quite ok, I found it unsatisfactory not to use so little data during training. And I would probably not have made it without the forums. 

# References
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013  

# Links
I spend so much time reading [R bloggers][1] and [Simply Statistics][2]!  
test [Read mode][1]

[1]: http://groupware.les.inf.puc-rio.br/har#ixzz3mmZGCXOK "data sets"
[2]: http://simplystatistics.org/ "Simply Statistics"  
